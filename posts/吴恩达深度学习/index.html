<!doctype html><html lang=zh-cn>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="noodp">
<title class=pjax-title>吴恩达深度学习 - traXe</title><meta name=Description content="这是Backtraxe的博客"><meta property="og:title" content="吴恩达深度学习">
<meta property="og:description" content>
<meta property="og:type" content="article">
<meta property="og:url" content="https://backtraxe.github.io/posts/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-03-28T00:00:00+00:00">
<meta property="article:modified_time" content="2021-03-28T00:00:00+00:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="吴恩达深度学习">
<meta name=twitter:description content>
<meta name=application-name content="traXe">
<meta name=apple-mobile-web-app-title content="traXe">
<meta name=theme-color content="#ffffff"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=canonical href=https://backtraxe.github.io/posts/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/><link rel=prev href=https://backtraxe.github.io/posts/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/><link rel=next href=https://backtraxe.github.io/posts/markdown-for-ppt/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload="this.onload=null,this.rel='stylesheet'" href=/lib/fontawesome-free/all.min.css>
<noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload="this.onload=null,this.rel='stylesheet'" href=/lib/animate/animate.min.css>
<noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"吴恩达深度学习","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/backtraxe.github.io\/posts\/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\/"},"genre":"posts","wordcount":1376,"url":"https:\/\/backtraxe.github.io\/posts\/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\/","datePublished":"2021-03-28T00:00:00+00:00","dateModified":"2021-03-28T00:00:00+00:00","publisher":{"@type":"Organization","name":"Backtraxe"},"author":{"@type":"Person","name":"Backtraxe"},"description":""}</script></head>
<body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(a){document.body.setAttribute('theme',a)}function saveTheme(a){window.localStorage&&localStorage.setItem('theme',a)}function getMeta(b){const a=document.getElementsByTagName('meta');for(let c=0;c<a.length;c++)if(a[c].getAttribute('name')===b)return a[c];return''}if(window.localStorage&&localStorage.getItem('theme')){let a=localStorage.getItem('theme');a==='light'||a==='dark'||a==='black'?setTheme(a):window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?setTheme('dark'):setTheme('light')}else''==='light'||''==='dark'||''==='black'?(setTheme(''),saveTheme('')):(saveTheme('auto'),window.matchMedia&&window.matchMedia('(prefers-color-scheme: dark)').matches?setTheme('dark'):setTheme('light'));let themeColorMeta=getMeta('theme-color');document.body.getAttribute('theme')!='light'&&(themeColorMeta.content='#000000')</script>
<div id=back-to-top></div>
<div id=mask></div><div class=wrapper><header class=desktop id=header-desktop>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title=traXe>traXe</a>
</div>
<div class=menu>
<div class=menu-inner><a class=menu-item href=/posts/> 文章 </a><a class=menu-item href=/tags/> 标签 </a><a class=menu-item href=/categories/> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=搜索>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=清空>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-desktop>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</span><a href=# onclick=return!1 class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a></div>
</div>
</div>
</header><header class=mobile id=header-mobile>
<div class=header-container>
<div class=header-wrapper>
<div class=header-title>
<a href=/ title=traXe>traXe</a>
</div>
<div class=menu-toggle id=menu-toggle-mobile>
<span></span><span></span><span></span>
</div>
</div>
<div class=menu id=menu-mobile><div class=search-wrapper>
<div class="search mobile" id=search-mobile>
<input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=搜索>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=清空>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-mobile>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</div>
<a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>
取消
</a>
</div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a href=# onclick=return!1 class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a></div>
</div>
</header>
<div class="search-dropdown desktop">
<div id=search-dropdown-desktop></div>
</div>
<div class="search-dropdown mobile">
<div id=search-dropdown-mobile></div>
</div>
<main class=main>
<div class=container><div class=toc id=toc-auto>
<h2 class=toc-title>目录</h2>
<div class=toc-content id=toc-content-auto></div>
</div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">吴恩达深度学习</h1><div class=post-meta>
<div class=post-meta-line>
<span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/ title=Author rel=" author" class=author>Backtraxe</a>
</span></div>
<div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2021-03-28>2021-03-28</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2021-03-28>2021-03-28</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 1376 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 3 分钟&nbsp;</div>
</div><div class="details toc" id=toc-static kept>
<div class="details-summary toc-title">
<span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span>
</div>
<div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents>
<ul>
<li><a href=#1-neural-networks-and-deep-learning>1 Neural Networks and Deep Learning</a>
<ul>
<li><a href=#11-introduction-to-deep-learning>1.1 Introduction to Deep Learning</a>
<ul>
<li><a href=#machine-learning-classification>Machine Learning Classification</a></li>
<li><a href=#data-classification>Data Classification</a></li>
</ul>
</li>
<li><a href=#12-basics-of-neural-network-programming>1.2 Basics of Neural Network programming</a>
<ul>
<li><a href=#math-notations>Math Notations</a></li>
<li><a href=#binary-classification-with-logistic-regression>Binary Classification with Logistic Regression</a></li>
</ul>
</li>
<li><a href=#13-one-hidden-layer-neural-networks>1.3 One hidden layer Neural Networks</a>
<ul>
<li><a href=#activation-function>Activation Function</a></li>
</ul>
</li>
<li><a href=#14-deep-neural-networks>1.4 Deep Neural Networks</a></li>
</ul>
</li>
<li><a href=#2-improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization>2 Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</a>
<ul>
<li><a href=#21-setting-up-your-ml-application>2.1 Setting up your ML application</a></li>
<li><a href=#22-optimization-algorithms>2.2 Optimization Algorithms</a></li>
<li><a href=#23-hyperparameter-tuning>2.3 Hyperparameter tuning</a></li>
</ul>
</li>
<li><a href=#3-structuring-your-machine-learning-project>3 Structuring your Machine Learning project</a></li>
<li><a href=#4-convolutional-neural-networks>4 Convolutional Neural Networks</a></li>
<li><a href=#5-natural-language-processing-building-sequence-models>5 Natural Language Processing: Building sequence models</a></li>
</ul>
</nav></div>
</div><div class=content id=content><h2 id=1-neural-networks-and-deep-learning>1 Neural Networks and Deep Learning</h2>
<h3 id=11-introduction-to-deep-learning>1.1 Introduction to Deep Learning</h3>
<p>![Simple Neural Networks](吴恩达深度学习.assets/屏幕截图 2020-09-18 165126.png)</p>
<h4 id=machine-learning-classification>Machine Learning Classification</h4>
<ul>
<li><strong>Supervised Learning</strong>: we already know the correct output called label.</li>
<li><strong>Unsupervised Learning</strong>: with no label.</li>
</ul>
<h4 id=data-classification>Data Classification</h4>
<ul>
<li><strong>Structued Data</strong>: Table, Records &mldr;</li>
<li><strong>Unstructured Data</strong>: Audio, Image, Text &mldr;</li>
</ul>
<h3 id=12-basics-of-neural-network-programming>1.2 Basics of Neural Network programming</h3>
<h4 id=math-notations>Math Notations</h4>
<ul>
<li>$m$: the number of training samples, $m_{train},m_{test}$.</li>
<li>$n$: the number of features.</li>
<li>$(x, y)$: all samples.</li>
<li>$(x^{(i)},y^{(i)})$: one sample, $x^{(i)}$ is a $(n \times 1)$ matrix, $y^{(i)}$ is a number.</li>
</ul>
<p>$$
x^{(i)}=\begin{bmatrix} x^{(i)}_1 \ x^{(i)}<em>2 \ &mldr; \ x^{(i)}</em>{n} \end{bmatrix}
$$</p>
<ul>
<li>$X$: training set, is a $(n \times m)$ matrix.</li>
</ul>
<p>$$
X=[x^{(1)},x^{(2)},&mldr;,x^{(i)},&mldr;,x^{(m)}]
$$</p>
<ul>
<li>$Y$: label set, is a $(1 \times m)$ matrix.</li>
</ul>
<p>$$
Y=\begin{bmatrix} y^{(1)}, y^{(2)},&mldr;,y^{(m)} \end{bmatrix}
$$</p>
<h4 id=binary-classification-with-logistic-regression>Binary Classification with Logistic Regression</h4>
<p>$\hat y=P(y=1|x), 0 \le \hat y \le 1$</p>
<p>Parameters:</p>
<ul>
<li>$w$: weights, a $(n \times 1)$ matrix.</li>
<li>$b$: threshold (bias), a $(1 \times m)$ matrix with all values are the same.</li>
</ul>
<p>$$
w=\begin{bmatrix} w_1 \ w_2 \ &mldr; \ w_n \end{bmatrix},
b=\begin{bmatrix} b_0,b_0,&mldr;,b_0 \end{bmatrix} \<br>
\hat y = \sigma(w^Tx+b)
$$</p>
<p><strong>Another Notation:</strong></p>
<p>Parameters:</p>
<ul>
<li>$\theta$: a $((n+1) \times 1)$ matrix.</li>
<li>$x^{(i)}$: a $((n+1) \times 1)$ matrix.</li>
<li>$X$: a $((n+1) \times m)$ matrix.</li>
</ul>
<p>$$
\theta = \begin{bmatrix} b_0 \ w_1 \ w_2 \ &mldr; \ w_n \end{bmatrix},
x^{(i)} = \begin{bmatrix} 1 \ x^{(i)}_1 \ x^{(i)}_2 \ &mldr; \ x^{(i)}_n \end{bmatrix},
X = \begin{bmatrix}
1 & 1 & &mldr; & 1 \<br>
x^{(1)}_1 & x^{(2)}_1 & &mldr; & x^{(m)}_1 \<br>
x^{(1)}_2 & x^{(2)}_2 & &mldr; & x^{(m)}_2 \<br>
&mldr; & &mldr; & &mldr; & &mldr; \<br>
x^{(1)}_n & x^{(2)}_n & &mldr; & x^{(m)}_n
\end{bmatrix}
$$</p>
<p>$$
\begin{aligned}
w^Tx^{(i)}+b_0
&= w_1 \cdot x^{(i)}_1 + w_2 \cdot x^{(i)}_2 + &mldr; + w_n \cdot x^{(i)}_n + b_0 \cdot 1 \<br>
&= \begin{bmatrix} b_0 \ w_1 \ w_2 \ &mldr; \ w_n \end{bmatrix}^T
\cdot \begin{bmatrix} 1 \ x^{(i)}_1 \ x^{(i)}_2 \ &mldr; \ x^{(i)}_n \end{bmatrix} \<br>
&= \theta^Tx^{(i)} \</p>
<p>w^Tx+b
&= \begin{bmatrix} w^Tx^{(1)},w^Tx^{(2)},&mldr;,w^Tx^{(m)} \end{bmatrix}</p>
<ul>
<li>\begin{bmatrix} b_0,b_0,&mldr;,b_0 \end{bmatrix} \<br>
&= \begin{bmatrix} w^Tx^{(1)}+b_0,w^Tx^{(2)}+b_0,&mldr;,w^Tx^{(m)}+b_0 \end{bmatrix} \<br>
&= \begin{bmatrix} \theta^Tx^{(1)},\theta^Tx^{(2)},&mldr;,\theta^Tx^{(m)} \end{bmatrix} \<br>
&= \theta^Tx
\end{aligned} \</li>
</ul>
<p>\hat y = \sigma(\theta^Tx)
$$</p>
<p><strong>Cost Function:</strong> error of all training examples.</p>
<p>$$
\begin{aligned}
\mathcal J(w,b)
&= \frac{1}{m}\sum_{i=1}^{m}\mathcal L(\hat y^{(i)},y^{(i)}) \<br>
&= -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log{(\hat y^{(i)})}+(1-y^{(i)})\log{(1-\hat y^{(i)})}]
\end{aligned}
$$</p>
<p>Reduction:</p>
<p>$$
If \ \ y=1: \ P(y|x)=\hat y \<br>
If \ \ y=0: \ P(y|x)=1-\hat y \<br>
Suppose \ P(y|x)=\hat y^y(1-\hat y)^{(1-y)} \<br>
Then \ \log{P(y|x)}=y\log{(\hat y)}+(1-y)\log{(1-\hat y)}
$$</p>
<p><strong>Loss (error) function:</strong> error of single training example.</p>
<p>$$
\mathcal L(\hat y,y)=-y\log{(\hat y)}-(1-y)\log{(1-\hat y)}
$$</p>
<p><strong>Gradient Descend:</strong></p>
<p>$$
w=w-\alpha\frac{\part \mathcal J(w,b)}{\part w} \<br>
b=b-\alpha\frac{\part \mathcal J(w,b)}{\part b}
$$</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>GradientDescend</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>learning_rate</span><span class=p>):</span>
    <span class=n>n_samples</span><span class=p>,</span> <span class=n>n_features</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>


<span class=n>n_samples</span><span class=p>,</span> <span class=n>n_features</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
<span class=n>J</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>w</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_features</span><span class=p>])</span>
<span class=n>dw</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_features</span><span class=p>])</span>
<span class=n>b</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>db</span> <span class=o>=</span> <span class=mi>0</span>
<span class=n>alpha</span> <span class=o>=</span> <span class=mf>0.05</span>

<span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>m</span><span class=p>):</span>
    <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>w</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>+</span> <span class=n>b</span>
    <span class=n>y_hat</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
    <span class=n>J</span> <span class=o>+=</span> <span class=o>-</span><span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>y_hat</span><span class=p>)</span> <span class=o>-</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>y_hat</span><span class=p>)</span>
    <span class=n>dw</span> <span class=o>=</span>
    <span class=n>db</span> <span class=o>=</span>
    <span class=n>w</span> <span class=o>-=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>dw</span>
    <span class=n>b</span> <span class=o>-=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>db</span>
</code></pre></td></tr></table>
</div>
</div><blockquote>
<p>Initialize weights to zero will make no sense, all weights in same layer will be the same.</p>
</blockquote>
<h3 id=13-one-hidden-layer-neural-networks>1.3 One hidden layer Neural Networks</h3>
<p>![2-layer Neural Network](吴恩达深度学习.assets/屏幕截图 2020-09-27 103709.png)</p>
<ul>
<li>Input Layer: We don&rsquo;t count input layer as an official layer.</li>
<li>Hidden Layer</li>
<li>Output Layer</li>
</ul>
<p>![Calculation in a neuron](吴恩达深度学习.assets/屏幕截图 2020-09-27 103212.png)</p>
<h4 id=activation-function>Activation Function</h4>
<p>Activation Function must be <code>nonlinear</code> function.</p>
<ul>
<li><strong>Sigmoid</strong></li>
</ul>
<p>$$
\begin{aligned}
\sigma(z) &= \frac{1}{1+e^{-z}} \<br>
\sigma'(z) &= \sigma(z)\cdot(1-\sigma(z))
\end{aligned}
$$</p>
<p>![Sigmoid](吴恩达深度学习.assets/屏幕截图 2020-09-18 171904.png)</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>sigmoid</span><span class=p>(</span><span class=n>z</span><span class=p>):</span>
    <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>z</span><span class=p>))</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>Tanh</strong></li>
</ul>
<p>$$
\begin{aligned}
\tanh(z) &= \frac{e^z-e^{-z}}{e^z+e^{-z}} \<br>
(\tanh(z))' &= 1-(\tanh(z))^2
\end{aligned}
$$</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>tanh</span><span class=p>(</span><span class=n>z</span><span class=p>):</span>
    <span class=k>return</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>z</span><span class=p>)</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>z</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>z</span><span class=p>)</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>z</span><span class=p>))</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>ReLU (Rectified Linear Unit)</strong>: Optimal.</li>
</ul>
<p>$$
g(z)=\max(0,z) \<br>
\begin{equation}
g'(z)=\begin{cases}
0, & \text{if} \ z \lt 0; \<br>
1, & \text{if} \ z \ge 0.
\end{cases}
\end{equation}
$$</p>
<p>![ReLU](吴恩达深度学习.assets/屏幕截图 2020-09-28 095206.png)</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>relu</span><span class=p>(</span><span class=n>z</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>z</span><span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><ul>
<li><strong>Leaky ReLU</strong></li>
</ul>
<p>$$
g(z)=\max(0.01z,z) \<br>
\begin{aligned}
g'(z)=\begin{cases}
0.01, & \text{if} \ z \lt 0; \<br>
1, & \text{if} \ z \ge 0.
\end{cases}
\end{aligned}
$$</p>
<p>![Leaky ReLU](吴恩达深度学习.assets/屏幕截图 2020-09-28 095353.png)</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=k>def</span> <span class=nf>leaky_relu</span><span class=p>(</span><span class=n>z</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=mf>0.01</span> <span class=o>*</span> <span class=n>z</span><span class=p>,</span> <span class=n>z</span><span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><h3 id=14-deep-neural-networks>1.4 Deep Neural Networks</h3>
<p>Forward propagation:</p>
<ul>
<li>Input: $a^{[l-1]}$</li>
<li>Output: $a^{[l]}$</li>
<li>Cache: $z^{[l]}, W^{[l]}, b^{[l]}$</li>
</ul>
<p>Backward propagation:</p>
<ul>
<li>Input: $da^{[l]}$</li>
<li>Output: $da^{[l-1]}, dW^{[l]}, db^{[l]}$</li>
</ul>
<p><strong>Parameters vs Hyperparameters</strong></p>
<ul>
<li>
<p><strong>Parameters:</strong> $W,b$</p>
</li>
<li>
<p><strong>Hyperparameters:</strong> $\alpha,iterations,hiddenLayers,hiddenUnits,activationFunctions$</p>
</li>
</ul>
<p>Optimal hyperparameters will change.</p>
<h2 id=2-improving-deep-neural-networks-hyperparameter-tuning-regularization-and-optimization>2 Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</h2>
<h3 id=21-setting-up-your-ml-application>2.1 Setting up your ML application</h3>
<p><strong>Datasets</strong></p>
<ul>
<li>Train sets:</li>
<li>Dev sets:</li>
<li>Test sets: evaluation</li>
</ul>
<p><strong>Bias and Variance</strong></p>
<ul>
<li>
<p>high variance: Dev set error &#187; Train set error.</p>
<p>More data/Regularization to solve.</p>
</li>
<li>
<p>high bias: Dev set error &#187; 0 & Train set error > 0.</p>
<p>Bigger network to solve.</p>
</li>
</ul>
<p><strong>Regularization</strong></p>
<ul>
<li>L1 Regularization</li>
</ul>
<p>$$
\mathcal J(w,b)=\frac{1}{m}\sum_{i=1}^m\mathcal L(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}\left| w \right|_1 \<br>
\left| w \right|_1=\sum_{j=1}^n|w_j|
$$</p>
<ul>
<li>L2 Regularization</li>
</ul>
<p>$$
\mathcal J(w,b)=\frac{1}{m}\sum_{i=1}^m\mathcal L(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}\left| w \right|_2^2 \<br>
\left| w \right|_2^2=\sum_{j=1}^nw_j^2=W^TW
$$</p>
<ul>
<li>$\lambda:$ regularization parameter</li>
</ul>
<p><strong>Dropout regularization</strong></p>
<ul>
<li>Inverted dropout</li>
</ul>
<p>keep-prop: the percentage of keeping neuron.</p>
<p><strong>Data augmentation</strong></p>
<ul>
<li>revolve</li>
<li>flip</li>
<li>zoom</li>
<li>distortion</li>
</ul>
<p><strong>Normalizing inputs</strong></p>
<ol>
<li>$x=x-\mu=x-\frac{1}{m}\sum_{i=1}^mx^{(i)}$</li>
<li>$x=\frac{x}{\sigma^2}=\frac{x}{\frac{1}{m}\sum_{i=1}^m(x^{(i)})^2}$</li>
</ol>
<p><strong>Vanishing/Exploding gradients</strong></p>
<p><strong>Weight initialization</strong></p>
<h3 id=22-optimization-algorithms>2.2 Optimization Algorithms</h3>
<p><strong>Mini-batch gradient descent</strong></p>
<p><strong>Stochastic gradient descent</strong></p>
<p><strong>Exponentially weighted averages</strong></p>
<p><strong>Gradient descent with momentum</strong></p>
<p>$$
\begin{aligned}
v_{dW}&=\beta v_{dW}+(1-\beta)dW \<br>
v_{db}&=\beta v_{db}+(1-\beta)db \<br>
W&=W-\alpha v_{dW} \<br>
b&=b-\alpha v_{db}
\end{aligned}
$$</p>
<p><strong>RMSprop (Root Mean Square prop)</strong></p>
<p>$$
\begin{aligned}
S_{dW}&=\beta S_{dW}+(1-\beta)dW^2 \<br>
S_{db}&=\beta S_{db}+(1-\beta)db^2 \<br>
W&=W-\alpha \frac{dW}{\sqrt{S_{dW}}} \<br>
b&=b-\alpha \frac{db}{\sqrt{S_{db}}}
\end{aligned}
$$</p>
<p><strong>Adam (Adaptive Moment Estimation) Optimization Algorithm</strong></p>
<p>$$
\begin{aligned}
v_{dW}&=\beta_1 v_{dW}+(1-\beta_1)dW \<br>
v_{db}&=\beta_1 v_{db}+(1-\beta_1)db \<br>
S_{dW}&=\beta_2 S_{dW}+(1-\beta_2)dW^2 \<br>
S_{db}&=\beta_2 S_{db}+(1-\beta_2)db^2 \<br>
v_{dW}^{corrected}&=\frac{v_{dW}}{1-\beta_1^t} \<br>
v_{db}^{corrected}&=\frac{v_{db}}{1-\beta_1^t} \<br>
S_{dW}^{corrected}&=\frac{S_{dW}}{1-\beta_2^t} \<br>
S_{db}^{corrected}&=\frac{S_{db}}{1-\beta_2^t} \<br>
W&=W-\alpha\frac{v_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\epsilon} \<br>
b&=b-\alpha\frac{v_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\epsilon}
\end{aligned}
$$</p>
<ul>
<li>$\alpha$: needs to be tune</li>
<li>$\beta_1$: 0.9</li>
<li>$\beta_2$: 0.999</li>
<li>$\epsilon: \ 10^{-8}$</li>
</ul>
<p><strong>Learning rate decay</strong></p>
<p>$$
\begin{aligned}
\alpha&=\frac{1}{1+decay_rate \times epoch_number}\alpha_0 \<br>
\alpha&=0.95^{epoch_number}\alpha_0 \<br>
\alpha&=\frac{k}{\sqrt{epoch_number}}\alpha_0
\end{aligned}
$$</p>
<h3 id=23-hyperparameter-tuning>2.3 Hyperparameter tuning</h3>
<p><strong>Hyperparameter advices</strong></p>
<ul>
<li>
<p>Try random values: don&rsquo;t use a grid.</p>
<p>We don&rsquo;t know which hyperparameter is more important.</p>
</li>
<li>
<p>Coarse to fine search.</p>
</li>
</ul>
<p><strong>Batch Norm</strong></p>
<p>$$
\begin{aligned}
\mu&=\frac{1}{m}\sum_iz^{(i)} \<br>
\sigma^2&=\frac{1}{m}\sum_i(z^{(i)}-\mu)^2 \<br>
z_{norm}^{(i)}&=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}} \<br>
\tilde z^{(i)}&=\gamma \cdot z_{norm}^{(i)}+\beta
\end{aligned}
$$</p>
<p><strong>Softmax Regression</strong></p>
<p>$$
\mathcal L(\hat y,y)=-\sum_{i=1}^Cy_i\log\hat y_i
$$</p>
<h2 id=3-structuring-your-machine-learning-project>3 Structuring your Machine Learning project</h2>
<p><strong>ML strategy</strong></p>
<ul>
<li>Collect more data</li>
<li>Collect more diverse training set</li>
<li>Train algorithm longer with gradient descent</li>
<li>Try Adam instead of gradient descent</li>
<li>Try bigger/smaller network</li>
<li>Try dropout</li>
<li>Add $L_2$ regularization</li>
<li>Network architecture
<ul>
<li>Activation functions</li>
<li>hidden units number</li>
</ul>
</li>
</ul>
<p><strong>Orthogonalization</strong></p>
<p>modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system.</p>
<ol>
<li>
<p>Fit training set well in cost function</p>
<p>If not, the use of a bigger neural network or switching to a better optimization algorithm might help.</p>
</li>
<li>
<p>Fit development set well on cost function</p>
<p>If not, regularization or using bigger training set might help.</p>
</li>
<li>
<p>Fit test set well on cost function</p>
<p>If not, the use of a bigger development set might help.</p>
</li>
<li>
<p>Performs well in real world</p>
<p>If not, the development test set is not set correctly or the cost function is not evaluating the right thing.</p>
</li>
</ol>
<p><strong>Evaluation Metrics</strong></p>
<table>
<thead>
<tr>
<th style=text-align:center></th>
<th style=text-align:center>Positive</th>
<th style=text-align:center>Negitive</th>
</tr>
</thead>
<tbody>
<tr>
<td style=text-align:center><strong>True</strong></td>
<td style=text-align:center>TP</td>
<td style=text-align:center>FN (Type 2)</td>
</tr>
<tr>
<td style=text-align:center><strong>False</strong></td>
<td style=text-align:center>FP (Type 1)</td>
<td style=text-align:center>TN</td>
</tr>
</tbody>
</table>
<ul>
<li>Accuracy: $Acc=\frac{TP+TN}{TP+TN+FP+FN}$</li>
<li>Precision (P): ratio of TRUE in test POSITIVE, $P=\frac{TP}{TP+FP}$</li>
<li>Recall (R): ratio of test POSITIVE in actually TRUE, $R=\frac{TP}{TP+FN}$</li>
<li>F1 Score: harmonic mean of P and R, $\frac{1}{F1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R}) \rightarrow F1=\frac{2PR}{P+R}$</li>
<li>F Score: $\frac{1}{F}=\frac{\beta^2}{1+\beta^2}(\frac{1}{P}+\frac{1}{R})$</li>
</ul>
<h2 id=4-convolutional-neural-networks>4 Convolutional Neural Networks</h2>
<h2 id=5-natural-language-processing-building-sequence-models>5 Natural Language Processing: Building sequence models</h2></div><div class=post-footer id=post-footer>
<div class=post-info>
<div class=post-info-line>
<div class=post-info-mod>
<span>更新于 2021-03-28</span>
</div>
<div class=post-info-license></div>
</div>
<div class=post-info-line>
<div class=post-info-md></div>
<div class=post-info-share>
<span></span>
</div>
</div>
</div>
<div class=post-info-more>
<section class=post-tags></section>
<section>
<span><a href=# onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span>
</section>
</div>
<div class=post-nav><a href=/posts/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/ class=prev rel=prev title=贝叶斯分类器><i class="fas fa-angle-left fa-fw"></i>贝叶斯分类器</a>
<a href=/posts/markdown-for-ppt/ class=next rel=next title="Markdown for PPT">Markdown for PPT<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
</article></div>
</main><footer class=footer>
<div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.87.0">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/HEIGE-PCloud/DoIt target=_blank rel="noopener noreffer" title="DoIt 0.2.10"><i class="far fa-edit fa-fw"></i> DoIt</a>
</div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank rel="noopener noreferrer">Backtraxe</a></span></div>
</div></footer></div>
<div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title=回到顶部>
<i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论>
<i class="fas fa-comment fa-fw"></i>
</a>
</div><div class=assets><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/lunr/lunr.min.js></script><script type=text/javascript src=/lib/lunr/lunr.stemmer.support.min.js></script><script type=text/javascript src=/lib/lunr/lunr.zh.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js></script></div>
<div class=pjax-assets><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/auto-render.min.js></script><script type=text/javascript src=/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:25},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",lunrLanguageCode:"zh",lunrSegmentitURL:"/lib/lunr/lunr.segmentit.js",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"lunr"}}</script><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css></div>
</body>
</html>